{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering with Llama 2\n",
    "\n",
    "Prompt engineering is using natural language to produce a desired response from a large language model (LLM).\n",
    "\n",
    "This interactive guide covers prompt engineering & best practices with Llama 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why now?\n",
    "\n",
    "[Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) introduced the world to transformer neural networks (originally for machine translation). Transformers ushered an era of generative AI with diffusion models for image creation and large language models (`LLMs`) as **programmable deep learning networks**.\n",
    "\n",
    "Programming foundational LLMs is done with natural language – it doesn't require training/tuning like ML models of the past. This has opened the door to a massive amount of innovation and a paradigm shift in how technology can be deployed. The science/art of using natural language to program language models to accomplish a task is referred to as **Prompt Engineering**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Models\n",
    "\n",
    "In 2023, Meta introduced the [Llama language models](https://ai.meta.com/llama/) (Llama Chat, Code Llama, Llama Guard). These are general purpose, state-of-the-art LLMs.\n",
    "\n",
    "Llama 2 models come in 7 billion, 13 billion, and 70 billion parameter sizes. Smaller models are cheaper to deploy and run (see: deployment and performance); larger models are more capable.\n",
    "\n",
    "#### Llama 2\n",
    "1. `llama-2-7b` - base pretrained 7 billion parameter model\n",
    "1. `llama-2-13b` - base pretrained 13 billion parameter model\n",
    "1. `llama-2-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-2-7b-chat` - chat fine-tuned 7 billion parameter model\n",
    "1. `llama-2-13b-chat` - chat fine-tuned 13 billion parameter model\n",
    "1. `llama-2-70b-chat` - chat fine-tuned 70 billion parameter model (flagship)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Llama is a code-focused LLM built on top of Llama 2 also available in various sizes and finetunes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Llama\n",
    "1. `codellama-7b` - code fine-tuned 7 billion parameter model\n",
    "1. `codellama-13b` - code fine-tuned 13 billion parameter model\n",
    "1. `codellama-34b` - code fine-tuned 34 billion parameter model\n",
    "1. `codellama-7b-instruct` - code & instruct fine-tuned 7 billion parameter model\n",
    "2. `codellama-13b-instruct` - code & instruct fine-tuned 13 billion parameter model\n",
    "3. `codellama-34b-instruct` - code & instruct fine-tuned 34 billion parameter model\n",
    "1. `codellama-7b-python` - Python fine-tuned 7 billion parameter model\n",
    "2. `codellama-13b-python` - Python fine-tuned 13 billion parameter model\n",
    "3. `codellama-34b-python` - Python fine-tuned 34 billion parameter model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting an LLM\n",
    "\n",
    "Large language models are deployed and accessed in a variety of ways, including:\n",
    "\n",
    "1. **Self-hosting**: Using local hardware to run inference. Ex. running Llama 2 on your Macbook Pro using [llama.cpp](https://github.com/ggerganov/llama.cpp).\n",
    "    * Best for privacy/security or if you already have a GPU.\n",
    "1. **Cloud hosting**: Using a cloud provider to deploy an instance that hosts a specific model. Ex. running Llama 2 on cloud providers like AWS, Azure, GCP, and others.\n",
    "    * Best for customizing models and their runtime (ex. fine-tuning a model for your use case).\n",
    "1. **Hosted API**: Call LLMs directly via an API. There are many companies that provide Llama 2 inference APIs including AWS Bedrock, Replicate, Anyscale, Together and others.\n",
    "    * Easiest option overall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hosted APIs\n",
    "\n",
    "Hosted APIs are the easiest way to get started. We'll use them here. There are usually two main endpoints:\n",
    "\n",
    "1. **`completion`**: generate a response to a given prompt (a string).\n",
    "1. **`chat_completion`**: generate the next message in a list of messages, enabling more explicit instruction and context for use cases like chatbots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "LLMs process inputs and outputs in chunks called *tokens*. Think of these, roughly, as words – each model will have its own tokenization scheme. For example, this sentence...\n",
    "\n",
    "> Our destiny is written in the stars.\n",
    "\n",
    "...is tokenized into `[\"our\", \"dest\", \"iny\", \"is\", \"written\", \"in\", \"the\", \"stars\"]` for Llama 2.\n",
    "\n",
    "Tokens matter most when you consider API pricing and internal behavior (ex. hyperparameters).\n",
    "\n",
    "Each model has a maximum context length that your prompt cannot exceed. That's 4096 tokens for Llama 2 and 100K for Code Llama. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "The following APIs will be used to call LLMs throughout the guide. As an example, we'll call Llama 2 chat using [Replicate](https://replicate.com/meta/llama-2-70b-chat) and use LangChain to easily set up a chat completion API.\n",
    "\n",
    "To install prerequisites run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\python\\python310\\lib\\site-packages (0.1.16)\n",
      "Requirement already satisfied: replicate in c:\\python\\python310\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\python\\python310\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\python\\python310\\lib\\site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\python\\python310\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\python\\python310\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\python\\python310\\lib\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\python\\python310\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in c:\\python\\python310\\lib\\site-packages (from langchain) (0.0.33)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in c:\\python\\python310\\lib\\site-packages (from langchain) (0.1.44)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\python\\python310\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\python\\python310\\lib\\site-packages (from langchain) (0.1.49)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\python\\python310\\lib\\site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hongweixli\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (1.10.9)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\python\\python310\\lib\\site-packages (from langchain) (2.26.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hongweixli\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: httpx<1,>=0.21.0 in c:\\python\\python310\\lib\\site-packages (from replicate) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\python\\python310\\lib\\site-packages (from replicate) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\python\\python310\\lib\\site-packages (from replicate) (4.11.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: anyio in c:\\python\\python310\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (3.6.2)\n",
      "Requirement already satisfied: certifi in c:\\python\\python310\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (2023.5.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python\\python310\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\python\\python310\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\python\\python310\\lib\\site-packages (from httpx<1,>=0.21.0->replicate) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.3)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\python\\python310\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.12)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python\\python310\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use GROQ instead of Replica\n",
    "import os\n",
    "#from getpass import getpass\n",
    "\n",
    "#GROQ_API_TOKEN = getpass()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "def llama2(prompt, temperature=0.0, top_p=0.9, input_print=True):\n",
    "  chat_completion = client.chat.completions.create(\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt,\n",
    "          }\n",
    "      ],\n",
    "      model=\"llama2-70b-4096\",\n",
    "      temperature=temperature,\n",
    "      top_p=top_p\n",
    "  )\n",
    "\n",
    "  return (chat_completion.choices[0].message.content)\n",
    "\n",
    "def llama3_8b(prompt, temperature=0.0, top_p=0.9, input_print=True):\n",
    "  chat_completion = client.chat.completions.create(\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt,\n",
    "          }\n",
    "      ],\n",
    "      model=\"llama3-8b-8192\",\n",
    "      temperature=temperature,\n",
    "      top_p=top_p\n",
    "  )\n",
    "\n",
    "  return (chat_completion.choices[0].message.content)\n",
    "\n",
    "def llama3_70b(prompt, temperature=0.0, top_p=0.9, input_print=True):\n",
    "  chat_completion = client.chat.completions.create(\n",
    "      messages=[\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": prompt,\n",
    "          }\n",
    "      ],\n",
    "      model=\"llama3-70b-8192\",\n",
    "      temperature=temperature,\n",
    "      top_p=top_p\n",
    "  )\n",
    "\n",
    "  return (chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langchain.llms import Replicate\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.schema.messages import get_buffer_string\n",
    "# We use GROQ instead of Replica\n",
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "# Get a free API key from https://replicate.com/account/api-tokens\n",
    "#os.environ[\"REPLICATE_API_TOKEN\"] = \"YOUR_KEY_HERE\"\n",
    "\n",
    "LLAMA2_70B_CHAT = \"meta/llama-2-70b-chat:2d19859030ff705a87c746f7e96eea03aefb71f166725aee39692f1476566d48\"\n",
    "LLAMA2_13B_CHAT = \"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\"\n",
    "\n",
    "# We'll default to the smaller 13B model for speed; change to LLAMA2_70B_CHAT for more advanced (but slower) generations\n",
    "#DEFAULT_MODEL = LLAMA2_13B_CHAT\n",
    "\n",
    "# We are using Llama3!!!\n",
    "DEFAULT_MODEL = \"llama3_8b\"\n",
    "\n",
    "\"\"\"\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    llm = Replicate(\n",
    "        model=model,\n",
    "        model_kwargs={\"temperature\": temperature,\"top_p\": top_p, \"max_new_tokens\": 1000}\n",
    "    )\n",
    "    return llm(prompt)\n",
    "\"\"\"\n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    if model == \"llama2\":\n",
    "        return llama2(prompt = prompt, temperature = temperature)\n",
    "    elif model == \"llama3_8b\":\n",
    "        return llama3_8b(prompt = prompt, temperature = temperature)\n",
    "    elif model == \"llama3_70b\":\n",
    "        return llama3_70b(prompt = prompt, temperature = temperature)\n",
    "    else:\n",
    "        print(\"Unknown model\")\n",
    "        return \"\"\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    model = DEFAULT_MODEL,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    history = ChatMessageHistory()\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            history.add_user_message(message[\"content\"])\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            history.add_ai_message(message[\"content\"])\n",
    "        else:\n",
    "            raise Exception(\"Unknown role\")\n",
    "    return completion(\n",
    "        get_buffer_string(\n",
    "            history.messages,\n",
    "            human_prefix=\"USER\",\n",
    "            ai_prefix=\"ASSISTANT\",\n",
    "        ),\n",
    "        model,\n",
    "        temperature,\n",
    "        top_p,\n",
    "    )\n",
    "\n",
    "def assistant(content: str):\n",
    "    return { \"role\": \"assistant\", \"content\": content }\n",
    "\n",
    "def user(content: str):\n",
    "    return { \"role\": \"user\", \"content\": content }\n",
    "\n",
    "def complete_and_print(prompt: str, model: str = DEFAULT_MODEL):\n",
    "    print(f'==============\\n{prompt}\\n==============')\n",
    "    response = completion(prompt, model)\n",
    "    print(response, end='\\n\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion APIs\n",
    "\n",
    "Llama 2 models tend to be wordy and explain their rationale. Later we'll explore how to manage the response length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "The typical color of the sky is: \n",
      "==============\n",
      "The typical color of the sky is blue!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"The typical color of the sky is: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "which model version are you?\n",
      "==============\n",
      "I am not a specific model version of a particular AI or machine learning model. I am a general-purpose conversational AI designed to understand and respond to natural language input. My responses are generated based on my training data, which includes a massive corpus of text from various sources, including books, articles, and online content.\n",
      "\n",
      "I'm an AI designed to assist and communicate with humans in a helpful and informative way. I don't have a specific model version or a specific purpose, but rather I'm a general-purpose conversational AI designed to be helpful and informative.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"which model version are you?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completion APIs\n",
    "Chat completion models provide additional structure to interacting with an LLM. An array of structured message objects is sent to the LLM instead of a single piece of text. This message list provides the LLM with some \"context\" or \"history\" from which to continue.\n",
    "\n",
    "Typically, each message contains `role` and `content`:\n",
    "* Messages with the `system` role are used to provide core instruction to the LLM by developers.\n",
    "* Messages with the `user` role are typically human-provided messages.\n",
    "* Messages with the `assistant` role are typically generated by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think I can recall that... Your favorite color is blue!\n"
     ]
    }
   ],
   "source": [
    "response = chat_completion(messages=[\n",
    "    user(\"My favorite color is blue.\"),\n",
    "    assistant(\"That's great to hear!\"),\n",
    "    user(\"What is my favorite color?\"),\n",
    "])\n",
    "print(response)\n",
    "# \"Sure, I can help you with that! Your favorite color is blue.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Hyperparameters\n",
    "\n",
    "#### `temperature` & `top_p`\n",
    "\n",
    "These APIs also take parameters which influence the creativity and determinism of your output.\n",
    "\n",
    "At each step, LLMs generate a list of most likely tokens and their respective probabilities. The least likely tokens are \"cut\" from the list (based on `top_p`), and then a token is randomly selected from the remaining candidates (`temperature`).\n",
    "\n",
    "In other words: `top_p` controls the breadth of vocabulary in a generation and `temperature` controls the randomness within that vocabulary. A temperature of ~0 produces *almost* deterministic results.\n",
    "\n",
    "[Read more about temperature setting here](https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683).\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature: 0.01 | top_p: 0.01]\n",
      "Fuzzy, gentle eyes\n",
      "Softly munching on grass\n",
      "Llama's gentle soul\n",
      "\n",
      "[temperature: 0.01 | top_p: 0.01]\n",
      "Fuzzy, gentle eyes\n",
      "Softly munching on grass\n",
      "Llama's gentle soul\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "Softly blending in\n",
      "Furry friends with gentle eyes\n",
      "Llama's gentle pace\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "Here is a haiku about llamas:\n",
      "\n",
      "Fuzzy, gentle beasts\n",
      "Softly munching on hay\n",
      "Majestic sight\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_tuned_completion(temperature: float, top_p: float):\n",
    "    response = completion(\"Write a haiku about llamas\", temperature=temperature, top_p=top_p)\n",
    "    print(f'[temperature: {temperature} | top_p: {top_p}]\\n{response.strip()}\\n')\n",
    "\n",
    "print_tuned_completion(0.01, 0.01)\n",
    "print_tuned_completion(0.01, 0.01)\n",
    "# These two generations are highly likely to be the same\n",
    "\n",
    "print_tuned_completion(1.0, 1.0)\n",
    "print_tuned_completion(1.0, 1.0)\n",
    "# These two generations are highly likely to be different"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Instructions\n",
    "\n",
    "Detailed, explicit instructions produce better results than open-ended prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Describe quantum physics in one short sentence of no more than 12 words\n",
      "==============\n",
      "\"Quantum physics: tiny particles behave randomly, defying classical understanding and logic.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(prompt=\"Describe quantum physics in one short sentence of no more than 12 words\")\n",
    "# Returns a succinct explanation of quantum physics that mentions particles and states existing simultaneously."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think about giving explicit instructions as using rules and restrictions to how Llama 2 responds to your prompt.\n",
    "\n",
    "- Stylization\n",
    "    - `Explain this to me like a topic on a children's educational network show teaching elementary students.`\n",
    "    - `I'm a software engineer using large language models for summarization. Summarize the following text in under 250 words:`\n",
    "    - `Give your answer like an old timey private investigator hunting down a case step by step.`\n",
    "- Formatting\n",
    "    - `Use bullet points.`\n",
    "    - `Return as a JSON object.`\n",
    "    - `Use less technical terms and help me apply it in my work in communications.`\n",
    "- Restrictions\n",
    "    - `Only use academic papers.`\n",
    "    - `Never give sources older than 2020.`\n",
    "    - `If you don't know the answer, say that you don't know.`\n",
    "\n",
    "Here's an example of giving explicit instructions to give more specific results by limiting the responses to recently created sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Explain the latest advances in large language models to me.\n",
      "==============\n",
      "Large language models have made tremendous progress in recent years, and I'd be happy to summarize the latest advances for you.\n",
      "\n",
      "**Transformer-based models**: The introduction of the Transformer architecture by Vaswani et al. (2017) revolutionized the field of natural language processing (NLP). Transformer-based models have become the de facto standard for many NLP tasks, including language translation, text classification, and question answering.\n",
      "\n",
      "**Scaling up**: One key aspect of recent progress is the ability to scale up model sizes and training datasets. This has been achieved through:\n",
      "\n",
      "1. **Model scaling**: Larger models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), have been trained on massive datasets and have achieved state-of-the-art results on various NLP tasks.\n",
      "2. **Data augmentation**: Techniques like masked language modeling (MLM) and next sentence prediction (NSP) have been used to create larger, more diverse training datasets.\n",
      "\n",
      "**Advances in architecture**: Researchers have introduced various architectural innovations to improve model performance and efficiency:\n",
      "\n",
      "1. **Multi-task learning**: Models like BERT and RoBERTa are trained on multiple tasks simultaneously, which helps them generalize better to unseen tasks.\n",
      "2. **Self-attention mechanisms**: Models like Transformer-XL (Dai et al., 2019) and Longformer (Beltagy et al., 2020) have introduced self-attention mechanisms to improve parallelization and reduce computational costs.\n",
      "3. **Knowledge distillation**: Techniques like knowledge distillation (KD) have been used to transfer knowledge from large models to smaller, more efficient ones.\n",
      "\n",
      "**Recent breakthroughs**:\n",
      "\n",
      "1. **T5 (Raffel et al., 2020)**: A text-to-text transformer model that can perform various NLP tasks, such as translation, summarization, and question answering.\n",
      "2. **DALL-E (Hudson & Manning, 2019)**: A generative model that can create realistic images from text descriptions.\n",
      "3. **LaMDA (Kaplan et al., 2020)**: A conversational AI model that can engage in natural-sounding conversations with humans.\n",
      "\n",
      "**Challenges and future directions**:\n",
      "\n",
      "1. **Explainability**: As models become more complex, there is a growing need for explainable AI (XAI) techniques to understand model decisions and improve transparency.\n",
      "2. **Adversarial attacks**: Researchers are working to develop robust models that can resist adversarial attacks and improve their security.\n",
      "3. **Multimodal learning**: Integrating multiple modalities (e.g., vision, audio, and text) into a single model is an active area of research.\n",
      "\n",
      "These are just a few highlights from the latest advances in large language models. As the field continues to evolve, we can expect to see even more exciting developments and applications in the years to come!\n",
      "\n",
      "==============\n",
      "Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\n",
      "==============\n",
      "Here are the latest advances in large language models:\n",
      "\n",
      "1. **Transformers and Attention Mechanisms**: The transformer architecture, introduced in the paper \"Attention Is All You Need\" by Vaswani et al. (2020), has become a standard component in many large language models. Transformers are particularly effective at modeling long-range dependencies in language.\n",
      "\n",
      "Source: Vaswani, A., Shazeer, N., Parmar, N., Johansson, V., Uszkoreit, J., & Nogueira dos Santos, C. (2020). Attention Is All You Need. Advances in Neural Information Processing Systems, 33.\n",
      "\n",
      "2. **Scaling Up Language Models**: Recent papers have demonstrated that scaling up language models by increasing their size and training them on larger datasets can lead to significant improvements in their performance.\n",
      "\n",
      "Source: Kaplan, A., et al. (2021). Scaling Laws for Neural Language Models. arXiv preprint arXiv:2102.10751.\n",
      "\n",
      "3. **Multitask Learning**: Multitask learning, where a single model is trained on multiple tasks simultaneously, has been shown to improve the performance of large language models on specific tasks.\n",
      "\n",
      "Source: Liu, Y., et al. (2021). Multitask Language Models for Natural Language Processing. arXiv preprint arXiv:2104.14421.\n",
      "\n",
      "4. **Knowledge Distillation**: Knowledge distillation, where a smaller model is trained to mimic the behavior of a larger model, has been used to improve the performance of large language models on specific tasks.\n",
      "\n",
      "Source: Jiao, X., et al. (2021). How to Fine-Tune BERT for Text Classification? arXiv preprint arXiv:2104.09609.\n",
      "\n",
      "5. **Efficient Inference**: Recent work has focused on improving the efficiency of large language models, including techniques such as pruning, quantization, and knowledge distillation.\n",
      "\n",
      "Source: Wang, Y., et al. (2021). Efficient Inference for Large Language Models. arXiv preprint arXiv:2104.14233.\n",
      "\n",
      "6. **Explainability and Interpretability**: As large language models become more widespread, there is a growing need for techniques to explain and interpret their predictions. Recent work has focused on developing methods for visualizing and understanding the behavior of these models.\n",
      "\n",
      "Source: Ribeiro, M. T., et al. (2021). Anchors: High-Precision Meaning Representation. arXiv preprint arXiv:2104.14344.\n",
      "\n",
      "7. **Adversarial Training**: Adversarial training, where a model is trained on data that is intentionally corrupted or perturbed, has been used to improve the robustness of large language models to adversarial attacks.\n",
      "\n",
      "Source: Zhang, Y., et al. (2021). Adversarial Training for Natural Language Processing. arXiv preprint arXiv:2104.14422.\n",
      "\n",
      "These are just a few examples of the latest advances in large language models. I hope this helps!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Explain the latest advances in large language models to me.\")\n",
    "# More likely to cite sources from 2017\n",
    "\n",
    "complete_and_print(\"Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\")\n",
    "# Gives more specific advances and only cites sources from 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Summarize the following text in under 50 words. \n",
      "beirut ( bayroot arabic romanized ) is the capital and largest city of lebanon. as of 2014 greater beirut has a population of 2.5 million which makes it the thirdlargest city in the levant region and the thirteenthlargest in the arab world. the city is situated on a peninsula at the midpoint of lebanons mediterranean coast. beirut has been inhabited for more than 5000 years making it one of the oldest cities in the world. beirut is lebanons seat of government and plays a central role in the lebanese economy with many banks and corporations based in the city. beirut is an important seaport for the country and region and rated a beta world city by the globalization and world cities research network. beirut was severely damaged by the lebanese civil war the 2006 lebanon war and the 2020 massive explosion in the port of beirut. its architectural and demographic structure underwent major change in recent decades. names the english name beirut is an early transcription of the arabic name bayrt (). the same names transcription into french is beyrouth which was sometimes used during lebanons french mandate. the arabic name derives from phoenician brt ( brt). this was a modification of the canaanite and phoenician word brt later brt meaning wells in reference to the sites accessible water table. the name is first attested in the 14th century bc when it was mentioned in three akkadian cuneiform tablets of the amarna letters letters sent by king ammunira of biruta to amenhotep iii or amenhotep iv of egypt. biruta was also mentioned in the amarna letters from king ribhadda of byblos. the greeks hellenised the name as bryts (ancient greek ) which the romans latinised as berytus. when it attained the status of a roman colony it was notionally refounded and its official name was emended to colonia iulia augusta felix berytus to include its imperial sponsors. at the time of the crusades the city was known in french as barut or baruth. prehistory beirut was settled over 5000 years ago and there is evidence that the surrounding area had already been inhabited for tens of thousands of years prior to this. several prehistoric archaeological sites have been discovered within the urban area of beirut revealing flint tools from sequential periods dating from the middle palaeolithic and upper paleolithic through the neolithic to the bronze age. beirut i (minet elhosn) was listed as the town of beirut (french beyrouth ville) by louis burkhalter and said to be on the beach near the orient and bassoul hotels on the avenue des franais in central beirut. the site was discovered by lortet in 1894 and discussed by godefroy zumoffen in 1900. the flint industry from the site was described as mousterian and is held by the museum of fine arts of lyon. beirut ii (umm elkhatib) was suggested by burkhalter to have been south of tarik el jedideh where p.e. gigues discovered a copper age flint industry at around 100 metres (328 feet) above sea level. the site had been built on and destroyed by 1948. beirut iii (furn eshshebbak) listed as plateau tabet was suggested to have been located on the left bank of the beirut river. burkhalter suggested that it was west of the damascus road although this determination has been criticized by lorraine copeland. p. e. gigues discovered a series of neolithic flint tools on the surface along with the remains of a structure suggested to be a hut circle. auguste bergy discussed polished axes that were also found at this site which has now completely disappeared as a result of construction and urbanization of the area. beirut iv (furn eshshebbak river banks) was also on the left bank of the river and on either side of the road leading eastwards from the furn esh shebbak police station towards the river that marked the city limits. the area was covered in red sand that represented quaternary river terraces. the site was found by jesuit father dillenseger and published by fellow jesuits godefroy zumoffen raoul describes and auguste bergy. \n",
      "\n",
      "==============\n",
      "Here is a summary of the text in under 50 words:\n",
      "\n",
      "Beirut is the capital and largest city of Lebanon, with a population of 2.5 million. It has a rich history, with evidence of human settlement dating back over 5,000 years. The city has been damaged by wars and has undergone significant changes in recent decades.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summerization_prompts = \"\"\"Summarize the following text in under 50 words, and use bullet points.\n",
    "beirut ( bayroot arabic romanized ) is the capital and largest city of lebanon. as of 2014 greater beirut has a population of 2.5 million which makes it the thirdlargest city in the levant region and the thirteenthlargest in the arab world. the city is situated on a peninsula at the midpoint of lebanons mediterranean coast. beirut has been inhabited for more than 5000 years making it one of the oldest cities in the world. beirut is lebanons seat of government and plays a central role in the lebanese economy with many banks and corporations based in the city. beirut is an important seaport for the country and region and rated a beta world city by the globalization and world cities research network. beirut was severely damaged by the lebanese civil war the 2006 lebanon war and the 2020 massive explosion in the port of beirut. its architectural and demographic structure underwent major change in recent decades. names the english name beirut is an early transcription of the arabic name bayrt (). the same names transcription into french is beyrouth which was sometimes used during lebanons french mandate. the arabic name derives from phoenician brt ( brt). this was a modification of the canaanite and phoenician word brt later brt meaning wells in reference to the sites accessible water table. the name is first attested in the 14th century bc when it was mentioned in three akkadian cuneiform tablets of the amarna letters letters sent by king ammunira of biruta to amenhotep iii or amenhotep iv of egypt. biruta was also mentioned in the amarna letters from king ribhadda of byblos. the greeks hellenised the name as bryts (ancient greek ) which the romans latinised as berytus. when it attained the status of a roman colony it was notionally refounded and its official name was emended to colonia iulia augusta felix berytus to include its imperial sponsors. at the time of the crusades the city was known in french as barut or baruth. prehistory beirut was settled over 5000 years ago and there is evidence that the surrounding area had already been inhabited for tens of thousands of years prior to this. several prehistoric archaeological sites have been discovered within the urban area of beirut revealing flint tools from sequential periods dating from the middle palaeolithic and upper paleolithic through the neolithic to the bronze age. beirut i (minet elhosn) was listed as the town of beirut (french beyrouth ville) by louis burkhalter and said to be on the beach near the orient and bassoul hotels on the avenue des franais in central beirut. the site was discovered by lortet in 1894 and discussed by godefroy zumoffen in 1900. the flint industry from the site was described as mousterian and is held by the museum of fine arts of lyon. beirut ii (umm elkhatib) was suggested by burkhalter to have been south of tarik el jedideh where p.e. gigues discovered a copper age flint industry at around 100 metres (328 feet) above sea level. the site had been built on and destroyed by 1948. beirut iii (furn eshshebbak) listed as plateau tabet was suggested to have been located on the left bank of the beirut river. burkhalter suggested that it was west of the damascus road although this determination has been criticized by lorraine copeland. p. e. gigues discovered a series of neolithic flint tools on the surface along with the remains of a structure suggested to be a hut circle. auguste bergy discussed polished axes that were also found at this site which has now completely disappeared as a result of construction and urbanization of the area. beirut iv (furn eshshebbak river banks) was also on the left bank of the river and on either side of the road leading eastwards from the furn esh shebbak police station towards the river that marked the city limits. the area was covered in red sand that represented quaternary river terraces. the site was found by jesuit father dillenseger and published by fellow jesuits godefroy zumoffen raoul describes and auguste bergy. \n",
    "\"\"\"\n",
    "complete_and_print(summerization_prompts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Prompting using Zero- and Few-Shot Learning\n",
    "\n",
    "A shot is an example or demonstration of what type of prompt and response you expect from a large language model. This term originates from training computer vision models on photographs, where one shot was one example or instance that the model used to classify an image ([Fei-Fei et al. (2006)](http://vision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf)).\n",
    "\n",
    "#### Zero-Shot Prompting\n",
    "\n",
    "Large language models like Llama 2 are unique because they are capable of following instructions and producing responses without having previously seen an example of a task. Prompting without examples is called \"zero-shot prompting\".\n",
    "\n",
    "Let's try using Llama 2 as a sentiment detector. You may notice that output format varies - we can improve this with better prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Text: This was the best movie I've ever seen! \n",
      " The sentiment of the text is: \n",
      "==============\n",
      "Positive\n",
      "\n",
      "==============\n",
      "Text: The director was trying too hard. \n",
      " The sentiment of the text is: \n",
      "==============\n",
      "Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Text: This was the best movie I've ever seen! \\n The sentiment of the text is: \")\n",
    "# Returns positive sentiment\n",
    "\n",
    "complete_and_print(\"Text: The director was trying too hard. \\n The sentiment of the text is: \")\n",
    "# Returns negative sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Few-Shot Prompting\n",
    "\n",
    "Adding specific examples of your desired output generally results in more accurate, consistent output. This technique is called \"few-shot prompting\".\n",
    "\n",
    "In this example, the generated response follows our desired format that offers a more nuanced sentiment classifer that gives a positive, neutral, and negative response confidence percentage.\n",
    "\n",
    "See also: [Zhao et al. (2021)](https://arxiv.org/abs/2102.09690), [Liu et al. (2021)](https://arxiv.org/abs/2101.06804), [Su et al. (2022)](https://arxiv.org/abs/2209.01975), [Rubin et al. (2022)](https://arxiv.org/abs/2112.08633).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    response = chat_completion(messages=[\n",
    "        user(\"You are a sentiment classifier. For each message, give the percentage of positive/netural/negative.\"),\n",
    "        user(\"I liked it\"),\n",
    "        assistant(\"70% positive 30% neutral 0% negative\"),\n",
    "        user(\"It could be better\"),\n",
    "        assistant(\"0% positive 50% neutral 50% negative\"),\n",
    "        user(\"It's fine\"),\n",
    "        assistant(\"25% positive 50% neutral 25% negative\"),\n",
    "        user(text),\n",
    "    ])\n",
    "    return response\n",
    "\n",
    "def print_sentiment(text):\n",
    "    print(f'INPUT: {text}')\n",
    "    print(sentiment(text))\n",
    "\n",
    "print_sentiment(\"I thought it was okay\")\n",
    "# More likely to return a balanced mix of positive, neutral, and negative\n",
    "print_sentiment(\"I loved it!\")\n",
    "# More likely to return 100% positive\n",
    "print_sentiment(\"Terrible service 0/10\")\n",
    "# More likely to return 100% negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role Prompting\n",
    "\n",
    "Llama 2 will often give more consistent responses when given a role ([Kong et al. (2023)](https://browse.arxiv.org/pdf/2308.07702.pdf)). Roles give context to the LLM on what type of answers are desired.\n",
    "\n",
    "Let's use Llama 2 to create a more focused, technical response for a question around the pros and cons of using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Explain the pros and cons of using PyTorch.\n",
      "==============\n",
      "PyTorch is a popular open-source machine learning library developed by Facebook's AI Research Lab (FAIR). Here are some pros and cons of using PyTorch:\n",
      "\n",
      "**Pros:**\n",
      "\n",
      "1. **Ease of use**: PyTorch has a simpler and more intuitive API compared to other deep learning frameworks like TensorFlow.\n",
      "2. **Dynamic computation graph**: PyTorch allows for dynamic computation graphs, which makes it easier to implement complex neural networks and modify them on the fly.\n",
      "3. **Automatic differentiation**: PyTorch's automatic differentiation feature makes it easy to compute gradients and perform backpropagation.\n",
      "4. **Pythonic**: PyTorch is built on top of Python, making it easy to integrate with other Python libraries and frameworks.\n",
      "5. **Flexibility**: PyTorch allows for flexible and custom implementation of neural networks, making it suitable for research and prototyping.\n",
      "6. **Large community**: PyTorch has a growing community of developers and researchers, which means there are many resources available for learning and troubleshooting.\n",
      "7. **GPU support**: PyTorch supports GPU acceleration, making it suitable for large-scale deep learning tasks.\n",
      "8. **Integration with other libraries**: PyTorch can be easily integrated with other popular libraries like NumPy, SciPy, and scikit-learn.\n",
      "\n",
      "**Cons:**\n",
      "\n",
      "1. **Steep learning curve**: While PyTorch is generally easier to learn than other deep learning frameworks, it still requires a good understanding of computer science and mathematics.\n",
      "2. **Limited support for distributed training**: PyTorch's distributed training capabilities are still evolving, making it less suitable for large-scale distributed training.\n",
      "3. **Less mature than TensorFlow**: PyTorch is still a relatively new framework, and while it has made significant progress, it still lags behind TensorFlow in terms of maturity and adoption.\n",
      "4. **Limited support for quantization**: PyTorch's support for quantization (reducing precision of model weights) is still limited compared to other frameworks like TensorFlow.\n",
      "5. **Limited support for model serving**: PyTorch's support for model serving (deploying models in production) is still limited, although it's improving.\n",
      "6. **Limited support for reinforcement learning**: PyTorch's support for reinforcement learning is still limited compared to other frameworks like TensorFlow.\n",
      "7. ** Limited support for large-scale production**: PyTorch is still not suitable for large-scale production environments due to its limited support for distributed training and model serving.\n",
      "8. **Less widely adopted**: PyTorch is still not as widely adopted as TensorFlow, which means there may be fewer resources available for learning and troubleshooting.\n",
      "\n",
      "In summary, PyTorch is a powerful and flexible deep learning framework that is well-suited for research and prototyping. However, it still has some limitations, particularly in terms of distributed training, model serving, and large-scale production environments.\n",
      "\n",
      "==============\n",
      "Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\n",
      "==============\n",
      "As a machine learning expert, I'm happy to provide an in-depth analysis of the pros and cons of using PyTorch, a popular open-source machine learning library.\n",
      "\n",
      "**Pros:**\n",
      "\n",
      "1. **Dynamic computation graph**: PyTorch allows for dynamic computation graphs, which means you can modify the computation graph at runtime. This flexibility is particularly useful for tasks like reinforcement learning, where the model needs to adapt to changing environments.\n",
      "2. **Pythonic API**: PyTorch's API is designed to feel like Python, making it easy to integrate with other Python libraries and frameworks. This is particularly useful for data scientists and engineers who are already familiar with Python.\n",
      "3. **Autograd**: PyTorch's autograd system automatically computes gradients for you, making it easy to implement backpropagation. This is particularly useful for deep learning models, where computing gradients by hand can be error-prone.\n",
      "4. **GPU support**: PyTorch has excellent support for GPU acceleration, making it suitable for large-scale deep learning applications.\n",
      "5. **Large community**: PyTorch has a large and active community, which means there are many resources available for learning and troubleshooting.\n",
      "6. **Extensive libraries and frameworks**: PyTorch has a wide range of libraries and frameworks available, including computer vision, natural language processing, and reinforcement learning libraries.\n",
      "\n",
      "**Cons:**\n",
      "\n",
      "1. **Steeper learning curve**: PyTorch's dynamic computation graph and autograd system can be overwhelming for beginners. It takes time to get comfortable with the PyTorch way of thinking.\n",
      "2. **Performance overhead**: PyTorch's dynamic computation graph can lead to performance overhead, especially for large models or complex computations. This can result in slower training times compared to static computation graphs.\n",
      "3. **Limited support for distributed training**: While PyTorch supports distributed training, it's not as seamless as other libraries like TensorFlow. This can make it more challenging to scale your models to large datasets or clusters.\n",
      "4. **Limited support for traditional machine learning**: PyTorch is primarily designed for deep learning. While it's possible to use PyTorch for traditional machine learning tasks, it may not be the best choice due to the overhead of the dynamic computation graph.\n",
      "5. ** Limited support for large-scale production**: While PyTorch is suitable for research and development, it may not be the best choice for large-scale production environments. PyTorch's dynamic computation graph and autograd system can make it challenging to optimize and deploy models in production environments.\n",
      "6. **Limited support for traditional software development**: PyTorch is primarily designed for machine learning tasks. While it's possible to use PyTorch for traditional software development, it may not be the best choice due to the overhead of the dynamic computation graph and autograd system.\n",
      "\n",
      "In conclusion, PyTorch is an excellent choice for deep learning tasks, particularly those that require dynamic computation graphs and autograd. However, it may not be the best choice for traditional machine learning tasks, large-scale production environments, or traditional software development. As a senior engineer working with complicated datasets, I would recommend PyTorch if:\n",
      "\n",
      "1. You're working on deep learning tasks that require dynamic computation graphs and autograd.\n",
      "2. You're comfortable with the PyTorch API and are willing to invest time in learning its unique features.\n",
      "3. You're working on research and development projects where the flexibility of PyTorch is beneficial.\n",
      "\n",
      "However, if you're working on traditional machine learning tasks, large-scale production environments, or traditional software development, you may want to consider alternative libraries and frameworks that better suit your needs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Explain the pros and cons of using PyTorch.\")\n",
    "# More likely to explain the pros and cons of PyTorch covers general areas like documentation, the PyTorch community, and mentions a steep learning curve\n",
    "\n",
    "complete_and_print(\"Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\")\n",
    "# Often results in more technical benefits and drawbacks that provide more technical details on how model layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought\n",
    "\n",
    "Simply adding a phrase encouraging step-by-step thinking \"significantly improves the ability of large language models to perform complex reasoning\" ([Wei et al. (2022)](https://arxiv.org/abs/2201.11903)). This technique is called \"CoT\" or \"Chain-of-Thought\" prompting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_and_print(\"Who lived longer Elvis Presley or Mozart?\")\n",
    "# Often gives incorrect answer of \"Mozart\"\n",
    "\n",
    "complete_and_print(\"Who lived longer Elvis Presley or Mozart? Let's think through this carefully, step by step.\")\n",
    "# Gives the correct answer \"Elvis\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Consistency\n",
    "\n",
    "LLMs are probablistic, so even with Chain-of-Thought, a single generation might produce incorrect results. Self-Consistency ([Wang et al. (2022)](https://arxiv.org/abs/2203.11171)) introduces enhanced accuracy by selecting the most frequent answer from multiple generations (at the cost of higher compute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from statistics import mode\n",
    "\n",
    "def gen_answer():\n",
    "    response = completion(\n",
    "        \"John found that the average of 15 numbers is 40.\"\n",
    "        \"If 10 is added to each number then the mean of the numbers is?\"\n",
    "        \"Report the answer surrounded by three backticks, for example: ```123```\",\n",
    "        model = LLAMA2_70B_CHAT\n",
    "    )\n",
    "    match = re.search(r'```(\\d+)```', response)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "answers = [gen_answer() for i in range(5)]\n",
    "\n",
    "print(\n",
    "    f\"Answers: {answers}\\n\",\n",
    "    f\"Final answer: {mode(answers)}\",\n",
    "    )\n",
    "\n",
    "# Sample runs of Llama-2-70B (all correct):\n",
    "# [50, 50, 750, 50, 50]  -> 50\n",
    "# [130, 10, 750, 50, 50] -> 50\n",
    "# [50, None, 10, 50, 50] -> 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "\n",
    "You'll probably want to use factual knowledge in your application. You can extract common facts from today's large models out-of-the-box (i.e. using just the model weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_and_print(\"What is the capital of the California?\", model = LLAMA2_70B_CHAT)\n",
    "# Gives the correct answer \"Sacramento\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, more specific facts, or private information, cannot be reliably retrieved. The model will either declare it does not know or hallucinate an incorrect answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_and_print(\"What was the temperature in Menlo Park on December 12th, 2023?\")\n",
    "# \"I'm just an AI, I don't have access to real-time weather data or historical weather records.\"\n",
    "\n",
    "complete_and_print(\"What time is my dinner reservation on Saturday and what should I wear?\")\n",
    "# \"I'm not able to access your personal information [..] I can provide some general guidance\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation, or RAG, describes the practice of including information in the prompt you've retrived from an external database ([Lewis et al. (2020)](https://arxiv.org/abs/2005.11401v4)). It's an effective way to incorporate facts into your LLM application and is more affordable than fine-tuning which may be costly and negatively impact the foundational model's capabilities.\n",
    "\n",
    "This could be as simple as a lookup table or as sophisticated as a [vector database]([FAISS](https://github.com/facebookresearch/faiss)) containing all of your company's knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MENLO_PARK_TEMPS = {\n",
    "    \"2023-12-11\": \"52 degrees Fahrenheit\",\n",
    "    \"2023-12-12\": \"51 degrees Fahrenheit\",\n",
    "    \"2023-12-13\": \"51 degrees Fahrenheit\",\n",
    "}\n",
    "\n",
    "\n",
    "def prompt_with_rag(retrived_info, question):\n",
    "    complete_and_print(\n",
    "        f\"Given the following information: '{retrived_info}', respond to: '{question}'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def ask_for_temperature(day):\n",
    "    temp_on_day = MENLO_PARK_TEMPS.get(day) or \"unknown temperature\"\n",
    "    prompt_with_rag(\n",
    "        f\"The temperature in Menlo Park was {temp_on_day} on {day}'\",  # Retrieved fact\n",
    "        f\"What is the temperature in Menlo Park on {day}?\",  # User question\n",
    "    )\n",
    "\n",
    "\n",
    "ask_for_temperature(\"2023-12-12\")\n",
    "# \"Sure! The temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\"\n",
    "\n",
    "ask_for_temperature(\"2023-07-18\")\n",
    "# \"I'm not able to provide the temperature in Menlo Park on 2023-07-18 as the information provided states that the temperature was unknown.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program-Aided Language Models\n",
    "\n",
    "LLMs, by nature, aren't great at performing calculations. Let's try:\n",
    "\n",
    "$$\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "$$\n",
    "\n",
    "(The correct answer is 91383.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_and_print(\"\"\"\n",
    "Calculate the answer to the following math problem:\n",
    "\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "\"\"\")\n",
    "# Gives incorrect answers like 92448, 92648, 95463"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gao et al. (2022)](https://arxiv.org/abs/2211.10435) introduced the concept of \"Program-aided Language Models\" (PAL). While LLMs are bad at arithmetic, they're great for code generation. PAL leverages this fact by instructing the LLM to write code to solve calculation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    # Python code to calculate: ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "    \"\"\",\n",
    "    model=\"meta/codellama-34b:67942fd0f55b66da802218a19a8f0e1d73095473674061a6ea19f2dc8c053152\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code was generated by Code Llama 34B:\n",
    "\n",
    "num1 = (-5 + 93 * 4 - 0)\n",
    "num2 = (4**4 + -7 + 0 * 5)\n",
    "answer = num1 * num2\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting Extraneous Tokens\n",
    "\n",
    "A common struggle is getting output without extraneous tokens (ex. \"Sure! Here's more information on...\").\n",
    "\n",
    "Check out this improvement that combines a role, rules and restrictions, explicit instructions, and an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_and_print(\n",
    "    \"Give me the zip code for Menlo Park in JSON format with the field 'zip_code'\",\n",
    "    model = LLAMA2_70B_CHAT,\n",
    ")\n",
    "# Likely returns the JSON and also \"Sure! Here's the JSON...\"\n",
    "\n",
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    You are a robot that only outputs JSON.\n",
    "    You reply in JSON format with the field 'zip_code'.\n",
    "    Example question: What is the zip code of the Empire State Building? Example answer: {'zip_code': 10118}\n",
    "    Now here is my question: What is the zip code of Menlo Park?\n",
    "    \"\"\",\n",
    "    model = LLAMA2_70B_CHAT,\n",
    ")\n",
    "# \"{'zip_code': 94025}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "- [PromptingGuide.ai](https://www.promptingguide.ai/)\n",
    "- [LearnPrompting.org](https://learnprompting.org/)\n",
    "- [Lil'Log Prompt Engineering Guide](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author & Contact\n",
    "\n",
    "Edited by [Dalton Flanagan](https://www.linkedin.com/in/daltonflanagan/) (dalton@meta.com) with contributions from Mohsen Agsen, Bryce Bortree, Ricardo Juan Palma Duran, Kaolin Fire, Thomas Scialom."
   ]
  }
 ],
 "metadata": {
  "captumWidgetMessage": [],
  "dataExplorerConfig": [],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "last_base_url": "https://bento.edge.x2p.facebook.net/",
  "last_kernel_id": "161e2a7b-2d2b-4995-87f3-d1539860ecac",
  "last_msg_id": "4eab1242-d815b886ebe4f5b1966da982_543",
  "last_server_session_id": "4a7b41c5-ed66-4dcb-a376-22673aebb469",
  "operator_data": [],
  "outputWidgetContext": []
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
