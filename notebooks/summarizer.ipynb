{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using llama-cpp-python to summarize a number of wiki articles\n",
    "\n",
    "## Installation\n",
    "1. Fork https://github.com/abetlen/llama-cpp-python/forkand clone       \n",
    "```\n",
    "git clone https://github.com/<your-git-id>/llama-cpp-python llama\n",
    "```\n",
    "2. Update the source tree (download llama-cpp)\n",
    "```\n",
    "cd llama\n",
    "git pull origin\n",
    "git submodule init\n",
    "git submodule update\n",
    "```\n",
    "3. Install the llama-cpp-python\n",
    "```\n",
    "python -m pip install --upgrade --force-reinstall --no-cache-dir .\n",
    "```\n",
    "\n",
    "## Coding the summarizer\n",
    "See following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub in c:\\python\\python310\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in c:\\python\\python310\\lib\\site-packages (from huggingface-hub) (3.12.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python\\python310\\lib\\site-packages (from huggingface-hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in c:\\python\\python310\\lib\\site-packages (from huggingface-hub) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\python\\python310\\lib\\site-packages (from huggingface-hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python\\python310\\lib\\site-packages (from huggingface-hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python\\python310\\lib\\site-packages (from huggingface-hub) (4.11.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\python\\python310\\lib\\site-packages (from huggingface-hub) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hongweixli\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python\\python310\\lib\\site-packages (from requests->huggingface-hub) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python310\\lib\\site-packages (from requests->huggingface-hub) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python\\python310\\lib\\site-packages (from requests->huggingface-hub) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python310\\lib\\site-packages (from requests->huggingface-hub) (3.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the llama2 model which is in pytorch .gguf format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model to directory \"./models\"\n",
    "from huggingface_hub import snapshot_download\n",
    "import pathlib\n",
    "\n",
    "model_id=\"TheBloke/Llama-2-7B-GGUF\"\n",
    "model_path = pathlib.Path(\"models/llama-2-7b-gguf\")\n",
    "if not model_path.exists():\n",
    "    model_path.mkdir(parents = True, exist_ok = True)\n",
    "    snapshot_download(repo_id=model_id, local_dir=\"models/llama-2-7B-gguf\",\n",
    "                    local_dir_use_symlinks=False, revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: '../vendor/llama.cpp/requirements.txt'\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r ../vendor/llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert .bin model to gguf model. Check out the ref link https://www.secondstate.io/articles/convert-pytorch-to-gguf/. But following two lines of code still don't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'c:\\\\projects\\\\llmplayground\\\\vendor\\\\llama.cpp\\\\convert.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python ../vendor/llama.cpp/convert.py models/llama-2-7b-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'c:\\\\projects\\\\llmplayground\\\\vendor\\\\llama.cpp\\\\convert-llama-ggml-to-gguf.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python ../vendor/llama.cpp/convert-llama-ggml-to-gguf.py --input ./models --output llama-2-7b-chat.ggmlv3.gguf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- https://github.com/tushitdave/Text_summarization/blob/main/Llama_2_Text_Summ.ipynb\n",
    "- https://medium.com/@tushitdavergtu/llama2-and-text-summarization-e3eafb51fe28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREEN = '\\033[92m'\n",
    "END_COLOR = '\\033[0m'\n",
    "\n",
    "import logging as logging\n",
    "from llama_cpp import Llama\n",
    "import torch as torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device_type, model_id, model_path, model_basename=None):\n",
    "\n",
    "    logging.info(f\"Loading Model: {model_id}, on: {device_type}\")\n",
    "    logging.info(\"This action can take a few minutes!\")\n",
    "\n",
    "    if model_basename is not None:\n",
    "        if \".ggml\" in model_basename:\n",
    "            logging.info(\"Using Llamacpp for GGML quantized models\")\n",
    "\n",
    "            max_ctx_size = 4096\n",
    "            kwargs = {\n",
    "                \"model_path\": model_path,\n",
    "                \"n_ctx\": max_ctx_size,\n",
    "                \"max_tokens\": max_ctx_size,\n",
    "            }\n",
    "            if device_type.lower() == \"mps\":\n",
    "                kwargs[\"n_gpu_layers\"] = 1000\n",
    "            if device_type.lower() == \"cuda\":\n",
    "                kwargs[\"n_gpu_layers\"] = 1000\n",
    "                kwargs[\"n_batch\"] = max_ctx_size\n",
    "            return Llama(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_TYPE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SHOW_SOURCES = True\n",
    "logging.info(f\"Running on: {DEVICE_TYPE}\")\n",
    "logging.info(f\"Display Source Documents set to: {SHOW_SOURCES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model path does not exist: ./models/llama-2-7b-gguf/llama-2-7b.Q4_0.gguf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Llama-2-7B-Chat-GGML\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m model_basename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-2-7b-chat.ggmlv3.q4_0.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m LLM \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE_TYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/llama-2-7b-gguf/llama-2-7b.Q4_0.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_basename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_basename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(device_type, model_id, model_path, model_basename)\u001b[0m\n\u001b[0;32m     19\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_gpu_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     20\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_ctx_size\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Llama(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\llama_cpp\\llama.py:320\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[1;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_path \u001b[38;5;241m=\u001b[39m lora_path\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m--> 320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m _LlamaModel(\n\u001b[0;32m    323\u001b[0m     path_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_path, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_params, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[0;32m    324\u001b[0m )\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Model path does not exist: ./models/llama-2-7b-gguf/llama-2-7b.Q4_0.gguf"
     ]
    }
   ],
   "source": [
    "model_id = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
    "model_basename = \"llama-2-7b-chat.ggmlv3.q4_0.bin\"\n",
    "LLM = load_model(device_type=DEVICE_TYPE, model_path=\"./models/llama-2-7b-gguf/llama-2-7b.Q4_0.gguf\", model_id=model_id, model_basename=model_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\python\\python310\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement transformers_longformer_tokenizer (from versions: none)\n",
      "ERROR: No matching distribution found for transformers_longformer_tokenizer\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers_longformer_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizer\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "\n",
    "def fetch_and_save_wiki_text(title):\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"explaintext\": True,\n",
    "        },\n",
    "    ).json()\n",
    "    \n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    wiki_text = page[\"extract\"]\n",
    "    \n",
    "    return wiki_text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters except \".\"\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s.\\(\\)\\[\\]\\{\\}]+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def count_tokens(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wonders_cities = [\n",
    "    'Beirut',\n",
    "    'Doha',\n",
    "    'Durban',\n",
    "    'Havana',\n",
    "    'Kuala Lumpur',\n",
    "    'La Paz',\n",
    "    'Vigan',\n",
    "]\n",
    "\n",
    "data = []\n",
    "for wonder_city in wonders_cities:\n",
    "    info = fetch_and_save_wiki_text(wonder_city)\n",
    "    tokens = tokenizer.encode(info, add_special_tokens=True, truncation=True, max_length=30000)\n",
    "    num_tokens = len(tokens)\n",
    "    data.append([wonder_city, info, num_tokens])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"wonder_city\", \"information\", \"num_tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wonder_city</th>\n",
       "      <th>information</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beirut</td>\n",
       "      <td>Beirut ( bay-ROOT; Arabic: بيروت, romanized: )...</td>\n",
       "      <td>12729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doha</td>\n",
       "      <td>Doha (Arabic: الدوحة, romanized: ad-Dawḥa [adˈ...</td>\n",
       "      <td>11117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Durban</td>\n",
       "      <td>Durban ( DUR-bən; Zulu: eThekwini, from itheku...</td>\n",
       "      <td>8350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Havana</td>\n",
       "      <td>Havana (; Spanish: La Habana [la aˈβana] ; Luc...</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Kuala Lumpur (Malaysian: [ˈkualə, -a ˈlumpo(r)...</td>\n",
       "      <td>12925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    wonder_city                                        information  num_tokens\n",
       "0        Beirut  Beirut ( bay-ROOT; Arabic: بيروت, romanized: )...       12729\n",
       "1          Doha  Doha (Arabic: الدوحة, romanized: ad-Dawḥa [adˈ...       11117\n",
       "2        Durban  Durban ( DUR-bən; Zulu: eThekwini, from itheku...        8350\n",
       "3        Havana  Havana (; Spanish: La Habana [la aˈβana] ; Luc...       30000\n",
       "4  Kuala Lumpur  Kuala Lumpur (Malaysian: [ˈkualə, -a ˈlumpo(r)...       12925"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (12083 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wonder_city</th>\n",
       "      <th>information</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>cleaned_information</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beirut</td>\n",
       "      <td>Beirut ( bay-ROOT; Arabic: بيروت, romanized: )...</td>\n",
       "      <td>12729</td>\n",
       "      <td>beirut ( bayroot arabic romanized ) is the cap...</td>\n",
       "      <td>12083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doha</td>\n",
       "      <td>Doha (Arabic: الدوحة, romanized: ad-Dawḥa [adˈ...</td>\n",
       "      <td>11117</td>\n",
       "      <td>doha (arabic romanized addawa [addua] or adda)...</td>\n",
       "      <td>10143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Durban</td>\n",
       "      <td>Durban ( DUR-bən; Zulu: eThekwini, from itheku...</td>\n",
       "      <td>8350</td>\n",
       "      <td>durban ( durbn zulu ethekwini from itheku mean...</td>\n",
       "      <td>7733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Havana</td>\n",
       "      <td>Havana (; Spanish: La Habana [la aˈβana] ; Luc...</td>\n",
       "      <td>30000</td>\n",
       "      <td>havana ( spanish la habana [la aana] lucumi il...</td>\n",
       "      <td>28948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>Kuala Lumpur (Malaysian: [ˈkualə, -a ˈlumpo(r)...</td>\n",
       "      <td>12925</td>\n",
       "      <td>kuala lumpur (malaysian [kual a lumpo(r) (r)])...</td>\n",
       "      <td>12674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    wonder_city                                        information  \\\n",
       "0        Beirut  Beirut ( bay-ROOT; Arabic: بيروت, romanized: )...   \n",
       "1          Doha  Doha (Arabic: الدوحة, romanized: ad-Dawḥa [adˈ...   \n",
       "2        Durban  Durban ( DUR-bən; Zulu: eThekwini, from itheku...   \n",
       "3        Havana  Havana (; Spanish: La Habana [la aˈβana] ; Luc...   \n",
       "4  Kuala Lumpur  Kuala Lumpur (Malaysian: [ˈkualə, -a ˈlumpo(r)...   \n",
       "\n",
       "   num_tokens                                cleaned_information  token_count  \n",
       "0       12729  beirut ( bayroot arabic romanized ) is the cap...        12083  \n",
       "1       11117  doha (arabic romanized addawa [addua] or adda)...        10143  \n",
       "2        8350  durban ( durbn zulu ethekwini from itheku mean...         7733  \n",
       "3       30000  havana ( spanish la habana [la aana] lucumi il...        28948  \n",
       "4       12925  kuala lumpur (malaysian [kual a lumpo(r) (r)])...        12674  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_information\"] = df[\"information\"].apply(clean_text)\n",
    "df[\"token_count\"] = df[\"cleaned_information\"].apply(count_tokens)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/ed/3e/93045d37eba24e0b5eb05312e30cd9e12805ea5f1ae9ba51ec8a7d2f5372/langchain-0.1.16-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\python\\python310\\lib\\site-packages (from langchain) (6.0)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Obtaining dependency information for SQLAlchemy<3,>=1.4 from https://files.pythonhosted.org/packages/29/82/3e4ca1381a3b0e80f03ba3fafbf047ed6c5f75ff4fd79f1726952c06f604/SQLAlchemy-2.0.29-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading SQLAlchemy-2.0.29-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Obtaining dependency information for aiohttp<4.0.0,>=3.8.3 from https://files.pythonhosted.org/packages/60/69/3febe2b4a12bc34721eb2ddb60b50d9e7fc8bdac98abb4019ffcd8032272/aiohttp-3.9.5-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Obtaining dependency information for async-timeout<5.0.0,>=4.0.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Obtaining dependency information for dataclasses-json<0.7,>=0.5.7 from https://files.pythonhosted.org/packages/91/ca/7219b838086086972e662c19e908694bdc6744537fb41b70392501b8b5e4/dataclasses_json-0.6.4-py3-none-any.whl.metadata\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
      "  Obtaining dependency information for langchain-community<0.1,>=0.0.32 from https://files.pythonhosted.org/packages/1f/cc/f65b573144bc95044354228760138a158dc09856beab8d178130591b2694/langchain_community-0.0.33-py3-none-any.whl.metadata\n",
      "  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
      "  Obtaining dependency information for langchain-core<0.2.0,>=0.1.42 from https://files.pythonhosted.org/packages/e9/ba/ee09e7613ff2162e15cb4b5b9181b4dae398298cbfe91bb6f431de59643e/langchain_core-0.1.44-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Obtaining dependency information for langchain-text-splitters<0.1,>=0.0.1 from https://files.pythonhosted.org/packages/9d/a1/aec824080111e9b4a4802b51b988032faa193828c865e11233d1b18e88fa/langchain_text_splitters-0.0.1-py3-none-any.whl.metadata\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Obtaining dependency information for langsmith<0.2.0,>=0.1.17 from https://files.pythonhosted.org/packages/7a/70/d05775b747745e94a09afd948672c88488c949f01d920ed920386098b697/langsmith-0.1.49-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.1.49-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\python\\python310\\lib\\site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hongweixli\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (1.10.9)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\python\\python310\\lib\\site-packages (from langchain) (2.26.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hongweixli\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (8.2.2)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl.metadata\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/61/15/2b5d644d81282f00b61e54f7b00a96f9c40224107282efe4cd9d2bf1433a/frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/ef/3d/ba0dc18e96c5d83731c54129819d5892389e180f54ebb045c6124b2e8b87/multidict-6.0.5-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/31/d4/2085272a5ccf87af74d4e02787c242c5d60367840a4637b2835565264302/yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.19.0)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.3)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
      "  Obtaining dependency information for packaging<24.0,>=23.2 from https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl.metadata\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Obtaining dependency information for orjson<4.0.0,>=3.9.14 from https://files.pythonhosted.org/packages/5b/40/42ee393448c5f77debc589b247f1400b51c5db95e906baa89d0fdd1ba0c2/orjson-3.10.1-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading orjson-3.10.1-cp310-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.9/50.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\python\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for mypy-extensions>=0.3.0 from https://files.pythonhosted.org/packages/2a/e2/5d3f6ada4297caebe1a2add3b126fe800c96f56dbe5d1988a2cbe0b267aa/mypy_extensions-1.0.0-py3-none-any.whl.metadata\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "   ---------------------------------------- 0.0/817.7 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 174.1/817.7 kB 5.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 245.8/817.7 kB 3.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 378.9/817.7 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 817.7/817.7 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-win_amd64.whl (370 kB)\n",
      "   ---------------------------------------- 0.0/370.7 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 184.3/370.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 370.7/370.7 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.33-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/1.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 1.0/1.9 MB 13.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/1.9 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.1.44-py3-none-any.whl (290 kB)\n",
      "   ---------------------------------------- 0.0/290.2 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 194.6/290.2 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 194.6/290.2 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 194.6/290.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 290.2/290.2 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading langsmith-0.1.49-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.2/115.2 kB ? eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.29-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.1 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.6/2.1 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.1 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.1 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 7.0 MB/s eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.4/50.4 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading orjson-3.10.1-cp310-none-win_amd64.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.1/139.1 kB ? eta 0:00:00\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/53.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 53.0/53.0 kB ? eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.4/76.4 kB ? eta 0:00:00\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: SQLAlchemy, packaging, orjson, mypy-extensions, multidict, jsonpatch, frozenlist, async-timeout, yarl, typing-inspect, langsmith, aiosignal, langchain-core, dataclasses-json, aiohttp, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "Successfully installed SQLAlchemy-2.0.29 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 dataclasses-json-0.6.4 frozenlist-1.4.1 jsonpatch-1.33 langchain-0.1.16 langchain-community-0.0.33 langchain-core-0.1.44 langchain-text-splitters-0.0.1 langsmith-0.1.49 multidict-6.0.5 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 typing-inspect-0.9.0 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\python\\python310\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.22.1 requires requests>=2.28, but you have requests 2.26.0 which is incompatible.\n",
      "nerfstudio 0.2.2 requires protobuf!=3.20.0,<=3.20.3, but you have protobuf 4.25.3 which is incompatible.\n",
      "nerfstudio 0.2.2 requires torch<2.0.0,>=1.12.1, but you have torch 2.1.2+cu121 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "def generate_summary(text_chunk, llm_model):\n",
    "    # Defining the template to generate summary\n",
    "    #template = \"\"\"\n",
    "    #Write a concise summary of the text, return your responses with 5 lines that cover the key points of the text.\n",
    "    #```{text}```\n",
    "    #SUMMARY:\n",
    "    #\"\"\"\n",
    "    #prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "    #llm_chain = LLMChain(prompt = prompt, llm = llm_model)\n",
    "\n",
    "    #summary = llm_chain.run(text_chunk)\n",
    "\n",
    "    prompts = \"\"\"\n",
    "       Write a concise summary of the text, return your responses with 5 lines that cover the key points of the text.\n",
    "        {}\n",
    "       SUMMARY:\n",
    "    \"\"\".format(text_chunk)\n",
    "\n",
    "    output = llm_model(\n",
    "      prompts, \n",
    "      max_tokens = 32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    "    ) # Generate a completion, can also call create_completion\n",
    "\n",
    "    summary = \"\"\n",
    "    res = output[\"choices\"][0][\"text\"]\n",
    "    start_index = res.find(\"SUMMARY:\")\n",
    "    if start_index != -1:\n",
    "      start_index += 8\n",
    "      summary = res[start_index:].strip()\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textsplitter in c:\\python\\python310\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: nltk>=3.8.1 in c:\\python\\python310\\lib\\site-packages (from textsplitter) (3.8.1)\n",
      "Requirement already satisfied: tiktoken>=0.3.0 in c:\\python\\python310\\lib\\site-packages (from textsplitter) (0.6.0)\n",
      "Requirement already satisfied: click in c:\\python\\python310\\lib\\site-packages (from nltk>=3.8.1->textsplitter) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\python\\python310\\lib\\site-packages (from nltk>=3.8.1->textsplitter) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python310\\lib\\site-packages (from nltk>=3.8.1->textsplitter) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\python\\python310\\lib\\site-packages (from nltk>=3.8.1->textsplitter) (4.65.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\python\\python310\\lib\\site-packages (from tiktoken>=0.3.0->textsplitter) (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.3.0->textsplitter) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.3.0->textsplitter) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.3.0->textsplitter) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.3.0->textsplitter) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hongweixli\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk>=3.8.1->textsplitter) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textsplitter\n",
    "!pip3 install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/llama-2-7b-gguf/llama-2-7b.Q4_0.gguf\n"
     ]
    }
   ],
   "source": [
    "print(LLM.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      4\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, length_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mdf\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating Summaries\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      9\u001b[0m     wonder_city \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwonder_city\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=50, length_function=len)\n",
    "\n",
    "df[\"summary\"] = \"\"\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Summaries\"):\n",
    "    wonder_city = row[\"wonder_city\"]\n",
    "    text_chunk = row[\"cleaned_information\"]\n",
    "    chunks = text_splitter.split_text(text_chunk)\n",
    "    chunk_summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        summary = generate_summary(chunk, LLM)\n",
    "        chunk_summaries.append(summary)\n",
    "\n",
    "    combined_summary = \"\\n\".join(chunk_summaries)\n",
    "    df.at[index, \"summary\"] = combined_summary\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I think that there are a lot of people who want to have a vacation in Lebanon, because it is very beautiful. I think it is\n"
     ]
    }
   ],
   "source": [
    "j = {'id': 'cmpl-d8c40893-068b-4955-a759-b1867b066d3c', 'object': 'text_completion', 'created': 1713749359, 'model': './models/llama-2-7b-gguf/llama-2-7b.Q4_0.gguf', 'choices': [{'text': '\\n       Q:Write a concise summary of the text, return your responses with 5 lines that cover the key points of the text.\\n        beirut ( bayroot arabic romanized ) is the capital and largest city of lebanon. as of 2014 greater beirut has a population of 2.5 million which makes it the thirdlargest city in the levant region and the thirteenthlargest in the arab world. the city is situated on a peninsula at the midpoint of lebanons mediterranean coast. beirut has been inhabited for more than 5000 years making it one of the oldest cities in the world. beirut is lebanons seat of government and plays a central role in the lebanese economy with many banks and corporations based in the city. beirut is an important seaport for the country and region and rated a beta world city by the globalization and world cities research network. beirut was severely damaged by the lebanese civil war the 2006 lebanon war and the 2020 massive explosion in the port of beirut. its architectural and demographic structure underwent major change in recent decades. names the english name beirut is an early transcription of the arabic name bayrt (). the same names transcription into french is beyrouth which was sometimes used during lebanons french mandate. the arabic name derives from phoenician brt ( brt). this was a modification of the canaanite and phoenician word brt later brt meaning wells in reference to the sites accessible water table. the name is first attested in the 14th century bc when it was mentioned in three akkadian cuneiform tablets of the amarna letters letters sent by king ammunira of biruta to amenhotep iii or amenhotep iv of egypt. biruta was also mentioned in the amarna letters from king ribhadda of byblos. the greeks hellenised the name as bryts (ancient greek ) which the romans latinised as berytus. when it attained the status of a roman colony it was notionally refounded and its official name was emended to colonia iulia augusta felix berytus to include its imperial sponsors. at the time of the crusades the city was known in french as barut or baruth. prehistory beirut was settled over 5000 years ago and there is evidence that the surrounding area had already been inhabited for tens of thousands of years prior to this. several prehistoric archaeological sites have been discovered within the urban area of beirut revealing flint tools from sequential periods dating from the middle palaeolithic and upper paleolithic through the neolithic to the bronze age. beirut i (minet elhosn) was listed as the town of beirut (french beyrouth ville) by louis burkhalter and said to be on the beach near the orient and bassoul hotels on the avenue des franais in central beirut. the site was discovered by lortet in 1894 and discussed by godefroy zumoffen in 1900. the flint industry from the site was described as mousterian and is held by the museum of fine arts of lyon. beirut ii (umm elkhatib) was suggested by burkhalter to have been south of tarik el jedideh where p.e. gigues discovered a copper age flint industry at around 100 metres (328 feet) above sea level. the site had been built on and destroyed by 1948. beirut iii (furn eshshebbak) listed as plateau tabet was suggested to have been located on the left bank of the beirut river. burkhalter suggested that it was west of the damascus road although this determination has been criticized by lorraine copeland. p. e. gigues discovered a series of neolithic flint tools on the surface along with the remains of a structure suggested to be a hut circle. auguste bergy discussed polished axes that were also found at this site which has now completely disappeared as a result of construction and urbanization of the area. beirut iv (furn eshshebbak river banks) was also on the left bank of the river and on either side of the road leading eastwards from the furn esh shebbak police station towards the river that marked the city limits. the area was covered in red sand that represented quaternary river terraces. the site was found by jesuit father dillenseger and published by fellow jesuits godefroy zumoffen raoul describes and auguste bergy. collections from the site were made by bergy describes and another jesuit paul\\n       A:\\n    1. I think that there are a lot of people who want to have a vacation in Lebanon, because it is very beautiful. I think it is', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 1106, 'completion_tokens': 32, 'total_tokens': 1138}}\n",
    "res = j[\"choices\"][0][\"text\"]\n",
    "start_index = res.find(\"A:\")\n",
    "if start_index != -1:\n",
    "    start_index += 2\n",
    "    print(res[start_index:].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wonder_city</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beirut</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doha</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Durban</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Havana</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>La Paz</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vigan</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    wonder_city                     summary\n",
       "0        Beirut  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\n",
       "1          Doha                            \n",
       "2        Durban                            \n",
       "3        Havana                            \n",
       "4  Kuala Lumpur                            \n",
       "5        La Paz                            \n",
       "6         Vigan                            "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"wonder_city\", \"summary\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
